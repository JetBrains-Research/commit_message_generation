dataset:
  dataset_root: raw_data/CleanedJiang
  diff_max_len: 110
  msg_max_len: 30
  encoder_name_or_path: microsoft/codebert-base
  decoder_name_or_path: distilgpt2
  train_dataloader_conf:
    batch_size: 32
    shuffle: true
    num_workers: 4
  val_dataloader_conf:
    batch_size: 32
    num_workers: 4
  test_dataloader_conf:
    batch_size: 32
    num_workers: 4
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  name: 2+2 codebert+distilgpt2
  project: commit_message_generation
model:
  learning_rate: 1e-4
  encoder_name_or_path: microsoft/codebert-base
  decoder_name_or_path: distilgpt2
  unfreeze_encoder_after: 20
  freeze_encoder_after: 5
  num_layers_encoder: 2
  num_layers_decoder: 2
trainer:
  gpus: 1
  max_epochs: 40
  num_sanity_val_steps: 0
  accumulate_grad_batches: 1

