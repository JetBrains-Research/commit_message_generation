dataset:
  dataset_root: raw_data/github_data
  history_max_len: 200
  encoder_name_or_path: microsoft/codebert-base
  decoder_name_or_path: distilgpt2
  local_rank: 0
  world_size: 0
  train_dataloader_conf:
    batch_size: 8
    num_workers: 1
  val_dataloader_conf:
    batch_size: 8
    num_workers: 1
  test_dataloader_conf:
    batch_size: 8
    num_workers: 1
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  name: codebert+distilgpt2 run without generation
  project: commit_message_generation
model:
  learning_rate: 1e-4
  num_layers_encoder: 6
  encoder_name_or_path: microsoft/codebert-base
  decoder_name_or_path: distilgpt2
trainer:
  gpus: 8
  accelerator: ddp
  max_epochs: 5
  precision: 16
  amp_level: O1
  auto_select_gpus: true
  num_sanity_val_steps: 100
  accumulate_grad_batches: 1
