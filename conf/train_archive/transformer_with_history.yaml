dataset:
  dataset_root: raw_data/multilang_v1

  training_with_history: true

  encoder_context_max_len: 500
  decoder_context_max_len: 200

  diff_tokenizer_name_or_path: raw_data/multilang_v1/diff_tokenizer.json
  msg_tokenizer_name_or_path: distilgpt2

  decoder_sep_tokens: "\n"

  train_dataloader_conf:
    batch_size: 16
    num_workers: 4

  val_dataloader_conf:
    batch_size: 16
    num_workers: 4

logger:
  name: transformer_multilang_v1 (random encoder + distilgpt2 decoder)
  project: commit_message_generation

model:
  encoder_decoder: true
  learning_rate: 1e-4
  num_layers_encoder: 6
  decoder_name_or_path: distilgpt2

artifact:
  name: transformer_distilgpt2
  type: multilang model

trainer:
  gpus: 4
  auto_select_gpus: false
  accelerator: ddp
  max_epochs: 5
  precision: 16
  amp_level: O1
  accumulate_grad_batches: 1
  num_sanity_val_steps: 100